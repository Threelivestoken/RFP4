{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c038f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import keras\n",
    "from keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "from gymnasium.wrappers import FrameStack\n",
    "from gymnasium.experimental.wrappers import GrayscaleObservationV0\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output\n",
    "%config NotebookApp.iopub_msg_rate_limit=10000\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43be4357",
   "metadata": {},
   "outputs": [],
   "source": [
    "totalEpisodesTrained = pd.read_csv('totalEpisodesTrained.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "173cdf4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.3</th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>episodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.3  Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  episodes\n",
       "0             0             0             0           0      8389"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalEpisodesTrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe108190",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ALE/Frogger-v5') # Human render mode slows things down A LOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21502e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = GrayscaleObservationV0(env) # Remove RGB channels (make gray) in order to decrease amount of data to process\n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2e644cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zache\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3444837047, 2669555309)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42 # Allows us to repeat the same patterns of game play\n",
    "env = FrameStack(env, 4) # Get 4 frames from game at a time\n",
    "frames, width, height = env.observation_space.shape\n",
    "env.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cafee4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 210 160\n"
     ]
    }
   ],
   "source": [
    "print(frames,width,height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bca17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This network learns an approximation of the Q-table, which is a mapping between the states and actions that an agent will take. \n",
    "# For every state we'll have FIVE actions that can be taken (NOOP, Up, Down, Left, Right). \n",
    "# The environment provides the state, and the action is chosen by selecting the largest of the five Q-values predicted in the output layer of the CNN.\n",
    "\n",
    "num_actions = 5\n",
    "\n",
    "# three convolution and three dense layers\n",
    "def create_CNN():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), padding='same', activation='relu', input_shape=(4, 210, 160)))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(num_actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "517c33db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zache\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# The first neural net makes the predictions for Q-values, which are used to take an action.\n",
    "cnn1 = create_CNN()\n",
    "if os.path.exists('cnn1v2.weights.h5'):\n",
    "    cnn1.load_weights('cnn1v2.weights.h5')\n",
    "\n",
    "# A second cnn is used to predict future rewards. The weights of the second cnn get updated every 10000 steps.\n",
    "cnn2 = create_CNN()\n",
    "if os.path.exists('cnn2v2.weights.h5'):\n",
    "    cnn2.load_weights('cnn2v2.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a18a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamaters DQN Algorithm\n",
    "\n",
    "gamma = 0.99  # Discount factor in Bellman's equuation\n",
    "epsilon = 1  # Epsilon greedy parameter for Q learning algorithm\n",
    "max_steps_per_episode = 75 #Deepmind trained for \"a total of 50 million frames (~38 days of game play)\"\n",
    "max_episodes = 10000  # Number of episodes you let the AI train. Keep above 1!\n",
    "epsilon_min = 0.1  # Smallest epsilon value possible\n",
    "epsilon_max = 1.0  # Largest epsilon value possible\n",
    "epsilon_interval = (epsilon_max - epsilon_min)  # Rate we reduce chance of random action being taken (eventually, we don't want to take many random actions)\n",
    "\n",
    "# Somre more important variables\n",
    "batch_size = 32  # Size of sample taken from \"replay buffer\"\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 50000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "# Maximum replay length\n",
    "# Note: A Deepmind paper suggests 1000000, however this can cause memory issues\n",
    "max_memory_length = 100000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 4\n",
    "# How often to update cnn2\n",
    "update_cnn2 = 10000\n",
    "# Using huber loss to check for convergance of Qs\n",
    "loss_function = keras.losses.Huber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cbddf0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4697/10000\n",
      "total episodes trained 13085\n",
      "time: 840.46 min\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 62\u001b[0m\n\u001b[0;32m     57\u001b[0m done_sample \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\n\u001b[0;32m     58\u001b[0m     [\u001b[38;5;28mfloat\u001b[39m(done_history[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices]\n\u001b[0;32m     59\u001b[0m )\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Build the updated Q-values for the sampled future states using cnn2\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m future_rewards \u001b[38;5;241m=\u001b[39m cnn2\u001b[38;5;241m.\u001b[39mpredict(state_next_sample)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Q value = reward + discount factor * expected future reward (Bellman's Equation)\u001b[39;00m\n\u001b[0;32m     64\u001b[0m updated_q_values \u001b[38;5;241m=\u001b[39m rewards_sample \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mamax(future_rewards, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:500\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    498\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_begin()\n\u001b[0;32m    499\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 500\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    502\u001b[0m         callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\contextlib.py:144\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m         \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:667\u001b[0m, in \u001b[0;36mTFEpochIterator.catch_stop_iteration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    666\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m--> 667\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_sync()\n\u001b[0;32m    668\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mStopIteration\u001b[39;00m, tf\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mOutOfRangeError):\n\u001b[0;32m    669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:660\u001b[0m, in \u001b[0;36mTFEpochIterator.tf_sync\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtf_sync\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 660\u001b[0m     tf_context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:2870\u001b[0m, in \u001b[0;36masync_wait\u001b[1;34m()\u001b[0m\n\u001b[0;32m   2868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context()\u001b[38;5;241m.\u001b[39m_context_handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 2870\u001b[0m   context()\u001b[38;5;241m.\u001b[39msync_executors()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:755\u001b[0m, in \u001b[0;36mContext.sync_executors\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Sync both local executors and the ones on remote workers.\u001b[39;00m\n\u001b[0;32m    744\u001b[0m \n\u001b[0;32m    745\u001b[0m \u001b[38;5;124;03mIn async execution mode, local function calls can return before the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;124;03m  ValueError: if context is not initialized.\u001b[39;00m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context_handle:\n\u001b[1;32m--> 755\u001b[0m   pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_ContextSyncExecutors(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context_handle)\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    757\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext is not initialized.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DQN Algorithm\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    observation, _ = env.reset()\n",
    "    state = np.array(observation)\n",
    "    episode_reward = 0\n",
    "    totalSteps = 0\n",
    "    \n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        frame_count += 1\n",
    "        # Use epsilon-greedy policy to explore or exploit\n",
    "            # If current frame_count is less than 50,000 or epsilon is greater than a random number between 0 and 1\n",
    "#         print(\"Frame Count: \", frame_count)\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict Q-values from environment state\n",
    "            state_tensor = keras.ops.convert_to_tensor(state) # Turn state (frame in game) into a Tensor Object (think matrix)\n",
    "            state_tensor = keras.ops.expand_dims(state_tensor, 0) # Add to the current batch\n",
    "            action_probs = cnn1(state_tensor, training=False)\n",
    "#             print(action_probs)\n",
    "            # Take best action\n",
    "            action = keras.ops.argmax(action_probs[0]).numpy()\n",
    "        \n",
    "        # Decrease probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "        \n",
    "        # Take action in environment\n",
    "#         print(\"Action: \", action)\n",
    "        state_next, reward, done, _, _ = env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "        \n",
    "        # Sum rewards across entire episode\n",
    "        totalSteps += 1\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save actions and states in \"replay buffer\"\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "        \n",
    "        # Update every fourth frame AND once batch size is greater than 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = keras.ops.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices]\n",
    "            )\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states using cnn2\n",
    "            future_rewards = cnn2.predict(state_next_sample)\n",
    "            # Q value = reward + discount factor * expected future reward (Bellman's Equation)\n",
    "            updated_q_values = rewards_sample + gamma * keras.ops.amax(future_rewards, axis=1)\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            # Create a 'mask' (matrix of 0s and 1s) so we only calculate loss on the updated Q-values\n",
    "            masks = keras.ops.one_hot(action_sample, num_actions)\n",
    "            \n",
    "            # Train the cnn1 using updated Q-values\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_values = cnn1(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = keras.ops.sum(keras.ops.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation - how cnn1 is updated\n",
    "            grads = tape.gradient(loss, cnn1.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, cnn1.trainable_variables))\n",
    "        \n",
    "        # Time to update cnn2?\n",
    "        if frame_count % update_cnn2 == 0:\n",
    "            # update cnn2 with new weights\n",
    "            cnn2.set_weights(cnn1.get_weights())\n",
    "            # Log details\n",
    "#             template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "            #print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "        \n",
    "        time.sleep(0.001)\n",
    "        clear_output(wait=True)\n",
    "        print(\"episode \" + str(episode_count + 1) + \"/\" + str(max_episodes))\n",
    "        print(\"total episodes trained \" + str(totalEpisodesTrained.loc[0, \"episodes\"]))\n",
    "        print(\"time: \" + str(round((time.time() - start_time)/60,2)) + \" min\")\n",
    "\n",
    "        if done:\n",
    "            print(\"DONE\")\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    s = episode_reward\n",
    "    x = totalSteps\n",
    "    actualScore = abs(10*(s/(x-(1.75*s))))\n",
    "    \n",
    "    if actualScore > s:\n",
    "        actualScore = s\n",
    "    \n",
    "    episode_reward_history.append(actualScore)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    \n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "#     print(\"Average Reward Across All Episodes: \", running_reward)\n",
    "    episode_count += 1\n",
    "\n",
    "    if running_reward > 40:  # Condition to consider game \"learned\"\n",
    "#         print(\"Learned at episode {}!\".format(episode_count))\n",
    "        break\n",
    "\n",
    "    if (max_episodes > 0 and episode_count >= max_episodes):  # Maximum number of episodes reached\n",
    "#         print(\"Stopped at episode {}!\".format(episode_count))\n",
    "        break\n",
    "    \n",
    "    # Save the weights of each CNN\n",
    "    totalEpisodesTrained.loc[0, \"episodes\"] += 1\n",
    "    totalEpisodesTrained.to_csv('totalEpisodesTrained.csv')\n",
    "    cnn1.save_weights(\"cnn1v2.weights.h5\")\n",
    "    cnn2.save_weights(\"cnn2v2.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_CNN()\n",
    "# model.load_weights('cnn1v2.weights.h5')\n",
    "# env = gym.make('ALE/Frogger-v5', render_mode=\"human\")\n",
    "# env = GrayscaleObservationV0(env)\n",
    "# env = FrameStack(env, 4) # Get 4 frames from game at a time\n",
    "# frames, width, height = env.observation_space.shape\n",
    "# env.reset()\n",
    "# game_over = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc7830e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# state, reward, game_over, x, _ = env.step(0)\n",
    "# while not game_over:\n",
    "    \n",
    "#     #Predict action using the trained model\n",
    "#     q_values = model.predict(np.expand_dims(state, axis=0))\n",
    "#     action = np.argmax(q_values)\n",
    "\n",
    "#     # Take action in the environment\n",
    "#     next_state, reward, game_over, x, _ = env.step(action)\n",
    "\n",
    "#     # Update current state\n",
    "#     state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb256b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
