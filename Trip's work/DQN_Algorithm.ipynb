{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c038f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import keras\n",
    "from keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "from gymnasium.wrappers import FrameStack\n",
    "from gymnasium.experimental.wrappers import GrayscaleObservationV0\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe108190",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ALE/Frogger-v5') # Human render mode slows things down A LOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21502e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = GrayscaleObservationV0(env) # Remove RGB channels (make gray) in order to decrease amount of data to process\n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2e644cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ejackson11\\AppData\\Roaming\\Python\\Python39\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3444837047, 2669555309)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42 # Allows us to repeat the same patterns of game play\n",
    "env = FrameStack(env, 4) # Get 4 frames from game at a time\n",
    "frames, width, height = env.observation_space.shape\n",
    "env.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3ed2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This network learns an approximation of the Q-table, which is a mapping between the states and actions that an agent will take. \n",
    "# For every state we'll have FIVE actions that can be taken (NOOP, Up, Down, Left, Right). \n",
    "# The environment provides the state, and the action is chosen by selecting the largest of the five Q-values predicted in the output layer of the CNN.\n",
    "\n",
    "num_actions = 5\n",
    "\n",
    "# three convolution and three dense layers\n",
    "def create_CNN():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, (8,8), strides=(4,4), padding='same', activation='relu', input_shape=(frames, width, height)))\n",
    "    model.add(Conv2D(64, (4,4), strides=(4,4), padding='same', activation='relu'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(num_actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "517c33db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first neural net makes the predictions for Q-values, which are used to take an action.\n",
    "cnn1 = create_CNN()\n",
    "\n",
    "# A second cnn is used to predict future rewards. The weights of the second cnn get updated every 10000 steps.\n",
    "cnn2 = create_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bca17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This network learns an approximation of the Q-table, which is a mapping between the states and actions that an agent will take. \n",
    "# For every state we'll have FIVE actions that can be taken (NOOP, Up, Down, Left, Right). \n",
    "# The environment provides the state, and the action is chosen by selecting the largest of the five Q-values predicted in the output layer of the CNN.\n",
    "\n",
    "num_actions = 5\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "def create_CNN():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, (8, 8), strides=(4,4), activation='relu', input_shape=(frames, width, height)))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(4,4), activation='relu'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(num_actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a18a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamaters DQN Algorithm\n",
    "\n",
    "gamma = 0.99  # Discount factor in Bellman's equuation\n",
    "epsilon = 1  # Epsilon greedy parameter for Q learning algorithm\n",
    "max_steps_per_episode = 75 #Deepmind trained for \"a total of 50 million frames (~38 days of game play)\"\n",
    "max_episodes = 1000  # Number of episodes you let the AI train. Keep above 1!\n",
    "epsilon_min = 0.1  # Smallest epsilon value possible\n",
    "epsilon_max = 1.0  # Largest epsilon value possible\n",
    "epsilon_interval = (epsilon_max - epsilon_min)  # Rate we reduce chance of random action being taken (eventually, we don't want to take many random actions)\n",
    "\n",
    "# Somre more important variables\n",
    "batch_size = 32  # Size of sample taken from \"replay buffer\"\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 50000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "# Maximum replay length\n",
    "# Note: A Deepmind paper suggests 1000000, however this can cause memory issues\n",
    "max_memory_length = 100000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 4\n",
    "# How often to update cnn2\n",
    "update_cnn2 = 10000\n",
    "# Using huber loss to check for convergance of Qs\n",
    "loss_function = keras.losses.Huber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbddf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Algorithm\n",
    "while True:\n",
    "    observation, _ = env.reset()\n",
    "    state = np.array(observation)\n",
    "    episode_reward = 0\n",
    "    totalSteps = 0\n",
    "    \n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        frame_count += 1\n",
    "        # Use epsilon-greedy policy to explore or exploit\n",
    "            # If current frame_count is less than 50,000 or epsilon is greater than a random number between 0 and 1\n",
    "        print(\"Frame Count: \", frame_count)\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict Q-values from environment state\n",
    "            state_tensor = keras.ops.convert_to_tensor(state) # Turn state (frame in game) into a Tensor Object (think matrix)\n",
    "            state_tensor = keras.ops.expand_dims(state_tensor, 0) # Add to the current batch\n",
    "            action_probs = cnn1(state_tensor, training=False)\n",
    "            print(action_probs)\n",
    "            # Take best action\n",
    "            action = keras.ops.argmax(action_probs[0]).numpy()\n",
    "        \n",
    "        # Decrease probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "        \n",
    "        # Take action in environment\n",
    "        print(\"Action: \", action)\n",
    "        state_next, reward, done, _, _ = env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "        \n",
    "        # Sum rewards across entire episode\n",
    "        totalSteps += 1\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save actions and states in \"replay buffer\"\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "        \n",
    "        # Update every fourth frame AND once batch size is greater than 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = keras.ops.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices]\n",
    "            )\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states using cnn2\n",
    "            future_rewards = cnn2.predict(state_next_sample)\n",
    "            # Q value = reward + discount factor * expected future reward (Bellman's Equation)\n",
    "            updated_q_values = rewards_sample + gamma * keras.ops.amax(future_rewards, axis=1)\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            # Create a 'mask' (matrix of 0s and 1s) so we only calculate loss on the updated Q-values\n",
    "            masks = keras.ops.one_hot(action_sample, num_actions)\n",
    "            \n",
    "            # Train the cnn1 using updated Q-values\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_values = cnn1(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = keras.ops.sum(keras.ops.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation - how cnn1 is updated\n",
    "            grads = tape.gradient(loss, cnn1.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, cnn1.trainable_variables))\n",
    "        \n",
    "        # Time to update cnn2?\n",
    "        if frame_count % update_cnn2 == 0:\n",
    "            # update cnn2 with new weights\n",
    "            cnn2.set_weights(cnn1.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "            #print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        if done:\n",
    "            print(\"DONE\")\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    s = episode_reward\n",
    "    x = totalSteps\n",
    "    actualScore = abs(10*(s/(x-(1.75*s))))\n",
    "    \n",
    "    if actualScore > s:\n",
    "        actualScore = s\n",
    "    \n",
    "    episode_reward_history.append(actualScore)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    \n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "    print(\"Average Reward Across All Episodes: \", running_reward)\n",
    "    episode_count += 1\n",
    "\n",
    "    if running_reward > 40:  # Condition to consider game \"learned\"\n",
    "        print(\"Learned at episode {}!\".format(episode_count))\n",
    "        break\n",
    "\n",
    "    if (max_episodes > 0 and episode_count >= max_episodes):  # Maximum number of episodes reached\n",
    "        print(\"Stopped at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1660e036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn1.save_weights(\"cnn1v2.weights.h5\") # Save the weights of each CNN\n",
    "#cnn2.save_weights(\"cnn2v2.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_CNN()\n",
    "model.load_weights('cnn1.weights.h5')\n",
    "env = gym.make('ALE/Frogger-v5', render_mode=\"human\")\n",
    "env = GrayscaleObservationV0(env)\n",
    "env = FrameStack(env, 4) # Get 4 frames from game at a time\n",
    "frames, width, height = env.observation_space.shape\n",
    "env.reset()\n",
    "game_over = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc7830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, game_over, x, _ = env.step(0)\n",
    "while not game_over:\n",
    "    \n",
    "    #Predict action using the trained model\n",
    "    q_values = model.predict(np.expand_dims(state, axis=0))\n",
    "    action = np.argmax(q_values)\n",
    "\n",
    "    # Take action in the environment\n",
    "    next_state, reward, game_over, x, _ = env.step(action)\n",
    "\n",
    "    # Update current state\n",
    "    state = next_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
