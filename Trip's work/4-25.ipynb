{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c038f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import keras\n",
    "from keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "from gymnasium.wrappers import FrameStack\n",
    "from gymnasium.experimental.wrappers import GrayscaleObservationV0\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe108190",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ALE/Frogger-v5') # Human render mode slows things down A LOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21502e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = GrayscaleObservationV0(env) # Remove RGB channels (make gray) in order to decrease amount of data to process\n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2e644cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ejackson11\\AppData\\Roaming\\Python\\Python39\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3444837047, 2669555309)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42 # Allows us to repeat the same patterns of game play\n",
    "env = FrameStack(env, 4) # Get 4 frames from game at a time\n",
    "frames, width, height = env.observation_space.shape\n",
    "env.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3ed2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This network learns an approximation of the Q-table, which is a mapping between the states and actions that an agent will take. \n",
    "# For every state we'll have FIVE actions that can be taken (NOOP, Up, Down, Left, Right). \n",
    "# The environment provides the state, and the action is chosen by selecting the largest of the five Q-values predicted in the output layer of the CNN.\n",
    "\n",
    "num_actions = 5\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "def create_CNN():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, (4, 4), strides=(4,4), activation='relu', input_shape=(frames, width, height)))\n",
    "    model.add(Conv2D(64, (8, 8), strides=(4,4), activation='relu'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(num_actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "517c33db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Computed output size would be negative. Received `inputs shape=(None, 1, 52, 32)`, `kernel shape=(8, 8, 32, 64)`, `dilation_rate=[1 1]`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30660\\2248068852.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# The first neural net makes the predictions for Q-values, which are used to take an action.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcnn1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_CNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# A second cnn is used to predict future rewards. The weights of the second cnn get updated every 10000 steps.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcnn2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_CNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30660\\2572293298.py\u001b[0m in \u001b[0;36mcreate_CNN\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\models\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer, rebuild)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrebuild\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_rebuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\models\\sequential.py\u001b[0m in \u001b[0;36m_maybe_rebuild\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInputLayer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_lock_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\layer.py\u001b[0m in \u001b[0;36mbuild_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mbuild_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_name_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m                 \u001b[0moriginal_build_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[1;31m# Record build config.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_build_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\models\\sequential.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m                 \u001b[1;31m# Can happen if shape inference is not implemented.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\ops\\operation_utils.py\u001b[0m in \u001b[0;36mcompute_conv_output_shape\u001b[1;34m(input_shape, filters, kernel_size, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_spatial_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnone_dims\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0moutput_spatial_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    222\u001b[0m                     \u001b[1;34m\"Computed output size would be negative. Received \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m                     \u001b[1;34mf\"`inputs shape={input_shape}`, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Computed output size would be negative. Received `inputs shape=(None, 1, 52, 32)`, `kernel shape=(8, 8, 32, 64)`, `dilation_rate=[1 1]`."
     ]
    }
   ],
   "source": [
    "# The first neural net makes the predictions for Q-values, which are used to take an action.\n",
    "cnn1 = create_CNN()\n",
    "\n",
    "# A second cnn is used to predict future rewards. The weights of the second cnn get updated every 10000 steps.\n",
    "cnn2 = create_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca17ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a18a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamaters DQN Algorithm\n",
    "\n",
    "gamma = 0.99  # Discount factor in Bellman's equuation\n",
    "epsilon = 1  # Epsilon greedy parameter for Q learning algorithm\n",
    "max_steps_per_episode = 75 #Deepmind trained for \"a total of 50 million frames (~38 days of game play)\"\n",
    "max_episodes = 1000  # Number of episodes you let the AI train. Keep above 1!\n",
    "epsilon_min = 0.1  # Smallest epsilon value possible\n",
    "epsilon_max = 1.0  # Largest epsilon value possible\n",
    "epsilon_interval = (epsilon_max - epsilon_min)  # Rate we reduce chance of random action being taken (eventually, we don't want to take many random actions)\n",
    "\n",
    "# Somre more important variables\n",
    "batch_size = 32  # Size of sample taken from \"replay buffer\"\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 50000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "# Maximum replay length\n",
    "# Note: A Deepmind paper suggests 1000000, however this can cause memory issues\n",
    "max_memory_length = 100000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 4\n",
    "# How often to update cnn2\n",
    "update_cnn2 = 10000\n",
    "# Using huber loss to check for convergance of Qs\n",
    "loss_function = keras.losses.Huber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cbddf0b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Count:  1\n",
      "Action:  1\n",
      "Frame Count:  2\n",
      "Action:  2\n",
      "Frame Count:  3\n",
      "Action:  3\n",
      "Frame Count:  4\n",
      "Action:  1\n",
      "Frame Count:  5\n",
      "Action:  2\n",
      "Frame Count:  6\n",
      "Action:  0\n",
      "Frame Count:  7\n",
      "Action:  3\n",
      "Frame Count:  8\n",
      "Action:  4\n",
      "Frame Count:  9\n",
      "Action:  2\n",
      "Frame Count:  10\n",
      "Action:  4\n",
      "Frame Count:  11\n",
      "Action:  4\n",
      "Frame Count:  12\n",
      "Action:  0\n",
      "Frame Count:  13\n",
      "Action:  3\n",
      "Frame Count:  14\n",
      "Action:  3\n",
      "Frame Count:  15\n",
      "Action:  4\n",
      "Frame Count:  16\n",
      "Action:  0\n",
      "Frame Count:  17\n",
      "Action:  1\n",
      "Frame Count:  18\n",
      "Action:  3\n",
      "Frame Count:  19\n",
      "Action:  2\n",
      "Frame Count:  20\n",
      "Action:  3\n",
      "Frame Count:  21\n",
      "Action:  3\n",
      "Frame Count:  22\n",
      "Action:  0\n",
      "Frame Count:  23\n",
      "Action:  1\n",
      "Frame Count:  24\n",
      "Action:  4\n",
      "Frame Count:  25\n",
      "Action:  4\n",
      "Frame Count:  26\n",
      "Action:  1\n",
      "Frame Count:  27\n",
      "Action:  3\n",
      "Frame Count:  28\n",
      "Action:  2\n",
      "Frame Count:  29\n",
      "Action:  4\n",
      "Frame Count:  30\n",
      "Action:  0\n",
      "Frame Count:  31\n",
      "Action:  2\n",
      "Frame Count:  32\n",
      "Action:  2\n",
      "Frame Count:  33\n",
      "Action:  2\n",
      "Frame Count:  34\n",
      "Action:  2\n",
      "Frame Count:  35\n",
      "Action:  0\n",
      "Frame Count:  36\n",
      "Action:  3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Conv2D.call().\n\n\u001b[1mNegative dimension size caused by subtracting 8 from 4 for '{{node sequential_1_1/conv2d_2_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 4, 4, 1], use_cudnn_on_gpu=true](sequential_1_1/Cast, sequential_1_1/conv2d_2_1/convolution/ReadVariableOp)' with input shapes: [32,4,210,160], [8,8,160,32].\u001b[0m\n\nArguments received by Conv2D.call():\n  • inputs=tf.Tensor(shape=(32, 4, 210, 160), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30660\\1743972791.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[1;31m# Build the updated Q-values for the sampled future states using cnn2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mfuture_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_next_sample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m             \u001b[1;31m# Q value = reward + discount factor * expected future reward (Bellman's Equation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mupdated_q_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards_sample\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuture_rewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Conv2D.call().\n\n\u001b[1mNegative dimension size caused by subtracting 8 from 4 for '{{node sequential_1_1/conv2d_2_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 4, 4, 1], use_cudnn_on_gpu=true](sequential_1_1/Cast, sequential_1_1/conv2d_2_1/convolution/ReadVariableOp)' with input shapes: [32,4,210,160], [8,8,160,32].\u001b[0m\n\nArguments received by Conv2D.call():\n  • inputs=tf.Tensor(shape=(32, 4, 210, 160), dtype=float32)"
     ]
    }
   ],
   "source": [
    "# DQN Algorithm\n",
    "while True:\n",
    "    observation, _ = env.reset()\n",
    "    state = np.array(observation)\n",
    "    episode_reward = 0\n",
    "    totalSteps = 0\n",
    "    \n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        frame_count += 1\n",
    "        # Use epsilon-greedy policy to explore or exploit\n",
    "            # If current frame_count is less than 50,000 or epsilon is greater than a random number between 0 and 1\n",
    "        print(\"Frame Count: \", frame_count)\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict Q-values from environment state\n",
    "            state_tensor = keras.ops.convert_to_tensor(state) # Turn state (frame in game) into a Tensor Object (think matrix)\n",
    "            state_tensor = keras.ops.expand_dims(state_tensor, 0) # Add to the current batch\n",
    "            action_probs = cnn1(state_tensor, training=False)\n",
    "            print(action_probs)\n",
    "            # Take best action\n",
    "            action = keras.ops.argmax(action_probs[0]).numpy()\n",
    "        \n",
    "        # Decrease probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "        \n",
    "        # Take action in environment\n",
    "        print(\"Action: \", action)\n",
    "        state_next, reward, done, _, _ = env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "        \n",
    "        # Sum rewards across entire episode\n",
    "        totalSteps += 1\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save actions and states in \"replay buffer\"\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "        \n",
    "        # Update every fourth frame AND once batch size is greater than 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = keras.ops.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices]\n",
    "            )\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states using cnn2\n",
    "            future_rewards = cnn2.predict(state_next_sample)\n",
    "            # Q value = reward + discount factor * expected future reward (Bellman's Equation)\n",
    "            updated_q_values = rewards_sample + gamma * keras.ops.amax(future_rewards, axis=1)\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            # Create a 'mask' (matrix of 0s and 1s) so we only calculate loss on the updated Q-values\n",
    "            masks = keras.ops.one_hot(action_sample, num_actions)\n",
    "            \n",
    "            # Train the cnn1 using updated Q-values\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_values = cnn1(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = keras.ops.sum(keras.ops.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation - how cnn1 is updated\n",
    "            grads = tape.gradient(loss, cnn1.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, cnn1.trainable_variables))\n",
    "        \n",
    "        # Time to update cnn2?\n",
    "        if frame_count % update_cnn2 == 0:\n",
    "            # update cnn2 with new weights\n",
    "            cnn2.set_weights(cnn1.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "            #print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        if done:\n",
    "            print(\"DONE\")\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    s = episode_reward\n",
    "    x = totalSteps\n",
    "    actualScore = abs(10*(s/(x-(1.75*s))))\n",
    "    \n",
    "    if actualScore > s:\n",
    "        actualScore = s\n",
    "    \n",
    "    episode_reward_history.append(actualScore)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    \n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "    print(\"Average Reward Across All Episodes: \", running_reward)\n",
    "    episode_count += 1\n",
    "\n",
    "    if running_reward > 40:  # Condition to consider game \"learned\"\n",
    "        print(\"Learned at episode {}!\".format(episode_count))\n",
    "        break\n",
    "\n",
    "    if (max_episodes > 0 and episode_count >= max_episodes):  # Maximum number of episodes reached\n",
    "        print(\"Stopped at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1660e036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn1.save_weights(\"cnn1v2.weights.h5\") # Save the weights of each CNN\n",
    "#cnn2.save_weights(\"cnn2v2.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_CNN()\n",
    "model.load_weights('cnn1.weights.h5')\n",
    "env = gym.make('ALE/Frogger-v5', render_mode=\"human\")\n",
    "env = GrayscaleObservationV0(env)\n",
    "env = FrameStack(env, 4) # Get 4 frames from game at a time\n",
    "frames, width, height = env.observation_space.shape\n",
    "env.reset()\n",
    "game_over = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc7830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, game_over, x, _ = env.step(0)\n",
    "while not game_over:\n",
    "    \n",
    "    #Predict action using the trained model\n",
    "    q_values = model.predict(np.expand_dims(state, axis=0))\n",
    "    action = np.argmax(q_values)\n",
    "\n",
    "    # Take action in the environment\n",
    "    next_state, reward, game_over, x, _ = env.step(action)\n",
    "\n",
    "    # Update current state\n",
    "    state = next_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
